@INPROCEEDINGS{9680403,
  author={Zarch, Mostafa Eghbali and Neff, Reece and Becchi, Michela},
  booktitle={2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)}, 
  title={Exploring Thread Coarsening on FPGA}, 
  year={2021},
  volume={},
  number={},
  pages={436-441},
  keywords={Codes;Instruction sets;Pipelines;Graphics processing units;Benchmark testing;Resource management;Synchronization;OpenCL;FPGA;high-level synthesis;compiler techniques;thread-coarsening;performance optimization},
  doi={10.1109/HiPC53243.2021.00062}}

@inproceedings{10.1145/3545008.3545067,
author = {Shah, Milan and Neff, Reece and Wu, Hancheng and Minutoli, Marco and Tumeo, Antonino and Becchi, Michela},
title = {Accelerating Random Forest Classification on GPU and FPGA},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545067},
doi = {10.1145/3545008.3545067},
abstract = {Random Forests (RFs) are a commonly used machine learning method for classification and regression tasks spanning a variety of application domains, including bioinformatics, business analytics, and software optimization. While prior work has focused primarily on improving performance of the training of RFs, many applications, such as malware identification, cancer prediction, and banking fraud detection, require fast RF classification. In this work, we accelerate RF classification on GPU and FPGA. In order to provide efficient support for large datasets, we propose a hierarchical memory layout suitable to the GPU/FPGA memory hierarchy. We design three RF classification code variants based on that layout, and we investigate GPU- and FPGA-specific considerations for these kernels. Our experimental evaluation, performed on an Nvidia Xp GPU and on a Xilinx Alveo U250 FPGA accelerator card using publicly available datasets on the scale of millions of samples and tens of features, covers various aspects. First, we evaluate the performance benefits of our hierarchical data structure over the standard compressed sparse row (CSR) format. Second, we compare our GPU implementation with cuML, a machine learning library targeting Nvidia GPUs. Third, we explore the performance/accuracy tradeoff resulting from the use of different tree depths in the RF. Finally, we perform a comparative performance analysis of our GPU and FPGA implementations. Our evaluation shows that, while reporting the best performance on GPU, our code variants outperform the CSR baseline both on GPU and FPGA. For high accuracy targets, our GPU implementation yields a 5-9 \texttimes{} speedup over CSR, and up to a 2 \texttimes{} speedup over Nvidia’s cuML library.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {4},
numpages = {11},
keywords = {FPGA, GPU, random forest classification},
location = {Bordeaux, France},
series = {ICPP '22}
}

@inproceedings{10.1145/3587135.3592196,
author = {Neff, Reece and Minutoli, Marco and Tumeo, Antonino and Becchi, Michela},
title = {High-Level Synthesis of Irregular Applications: A Case Study on Influence Maximization},
year = {2023},
isbn = {9798400701405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587135.3592196},
doi = {10.1145/3587135.3592196},
abstract = {FPGAs are promising platforms for accelerating irregular applications due to their ability to implement highly specialized hardware designs for each kernel. However, the design and implementation of FPGA-accelerated kernels can take several months using hardware design languages. High Level Synthesis (HLS) tools provide fast, high quality results for regular applications, but lack the support to effectively accelerate more irregular, complex workloads. This work analyzes the challenges and benefits of using a commercial state-of-the-art HLS tool and its available optimizations to accelerate graph sampling. We evaluate the resulting designs and their effectiveness when deployed in a state-of-the-art heterogeneous framework that implements the Influence Maximization with Martingales (IMM) algorithm, a complex graph analytics algorithm. We discuss future opportunities for improvement in hardware, HLS tools, and hardware/software co-design methodology to better support complex irregular applications such as IMM.},
booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers},
pages = {12–22},
numpages = {11},
keywords = {Graph Algorithms, High-Level Synthesis, Influence Maximization},
location = {Bologna, Italy},
series = {CF '23}
}

@INPROCEEDINGS{10579092,
  author={Ferdous, S M and Neff, Reece and Peng, Bo and Shuvo, Salman and Minutoli, Marco and Mukherjee, Sayak and Kowalski, Karol and Becchi, Michela and Halappanavar, Mahantesh},
  booktitle={2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Picasso: Memory-Efficient Graph Coloring Using Palettes With Applications in Quantum Computing}, 
  year={2024},
  volume={},
  number={},
  url={https://doi.org/10.1109/IPDPS57955.2024.00029},
  pages={241-252},
  keywords={Distributed processing;Machine learning algorithms;Quantum computing;Graphics processing units;Color;Machine learning;Predictive models;Graph coloring;quantum computing;memory-efficient algorithms},
  doi={10.1109/IPDPS57955.2024.00029}}

@inproceedings{10.1145/3650200.3656621,
author = {Neff, Reece and Zarch, Mostafa Eghbali and Minutoli, Marco and Halappanavar, Mahantesh and Tumeo, Antonino and Kalyanaraman, Ananth and Becchi, Michela},
title = {FuseIM: Fusing Probabilistic Traversals for Influence Maximization on Exascale Systems},
year = {2024},
isbn = {9798400706103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650200.3656621},
doi = {10.1145/3650200.3656621},
abstract = {Probabilistic breadth-first traversals (BPTs) are used in many network science and graph machine learning applications. In this paper, we are motivated by the application of BPTs in stochastic diffusion-based graph problems such as influence maximization. These applications heavily rely on BPTs to implement a Monte-Carlo sampling step for their approximations. Given the large sampling complexity, stochasticity of the diffusion process, and the inherent irregularity in real-world graph topologies, efficiently parallelizing these BPTs remains significantly challenging. In this paper, we present a new algorithm to fuse a massive number of concurrently executing BPTs with random starts on the input graph. Our algorithm is designed to fuse BPTs by combining separate probabilistic traversals into a unified frontier. To show the general applicability of the fused BPT technique, we have incorporated it into two state-of-the-art influence maximization parallel implementations (gIM and Ripples). Our experiments on up to 4K nodes of the OLCF Frontier supercomputer (32,768 GPUs and 196K CPU cores) show strong scaling behavior, and that fused BPTs can improve the performance of these implementations up to 182.13\texttimes{} (avg. 75.15\texttimes{}) and 359.86\texttimes{} (avg. 135.17\texttimes{}) for gIM and Ripples, respectively.},
booktitle = {Proceedings of the 38th ACM International Conference on Supercomputing},
pages = {38–49},
numpages = {12},
keywords = {Influence Maximization, Parallel Graph Algorithms},
location = {Kyoto, Japan},
series = {ICS '24},
note = {Best Paper Nominee}
}

@inproceedings{minutoli2025dimples,
  author    = {Marco Minutoli and Reece Neff and Naw Safrin Sattar and Hao Lu and John Feo and Henning Mortveit and Anil Vullikanti and Dawen Xie and Mandy L. Wilson and Gregor von Laszewski and Parantapa Bhattacharya and S M Ferdous and Ananth Kalyanaraman and Michela Becchi and Madhav Marathe and Mahantesh Halappanavar},
  title     = {DIMPLES: Distributed Influence Maximization for Pandemic pLanning on Exascale Systems},
  booktitle = {To appear in ICS '25, June 8--11},
  location = {Salt Lake City, UT, USA},
  year      = {2025},
  note      = {Best Paper Nominee}
}